# titanic-kaggle-ML-competition

![challenge ranking](https://user-images.githubusercontent.com/55531728/120501823-807c5300-c3f4-11eb-8ca6-6676e4b01aba.png)

In this competition I did a lot of data analysis, data preprocessing, data cleaning, and feature engineering. I did many different scenarios where I tried techniques such as standardize the data, normalize the data, using dummy variables through one-hot encoding, and used a different number of features based on their correlation with the target variable.
Then I tried many machine learning models such as Random Forest, Logistic Regression, XGBoost, SVM, and Decision tree. Finally, just because of curiosity, I also tried a deep learning model using a MLP.

Long story short the best model was Random Forest, and the worst was SVM. That is why I did not include the code of SVM in the two notebooks I uploaded. 
1. "best_accuracy_code.ipynb" includes the code of the best Random Forest model, plus all the different scenarios and machine learning models as comments.
2. "mlp.ipynb" includes the code of the deep learning model Multi-Layer Perceptron.

The link of the competition is here https://www.kaggle.com/c/titanic/overview, where you can also find the training and testing set used for the challenge.
